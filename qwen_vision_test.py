"""
æµ‹è¯•Qwen2.5-VLè§†è§‰ç¼–ç å™¨å¯¼å…¥å’ŒåŸºæœ¬åŠŸèƒ½
"""

import torch
import torch.nn as nn
from transformers import AutoConfig, AutoImageProcessor
import safetensors
from PIL import Image
import numpy as np
import os
import sys
import json

def load_qwen_vision_config():
    """åŠ è½½Qwen2.5-VLçš„é…ç½®"""
    config_path = "/home/Qwen2.5-VL-3B-Instruct/config.json"
    with open(config_path, 'r') as f:
        config = json.load(f)
    return config

def create_qwen_vision_encoder(config):
    """æ ¹æ®é…ç½®åˆ›å»ºQwenè§†è§‰ç¼–ç å™¨æ¶æ„"""
    vision_config = config['vision_config']
    
    class QwenVisionEncoder(nn.Module):
        def __init__(self, vision_config):
            super().__init__()
            # ç®€åŒ–çš„è§†è§‰ç¼–ç å™¨ï¼Œåªæµ‹è¯•åŸºæœ¬åŠŸèƒ½
            self.patch_embed = nn.Conv2d(
                in_channels=vision_config['in_chans'],
                out_channels=vision_config['hidden_size'],
                kernel_size=vision_config['patch_size'],
                stride=vision_config['patch_size']
            )
            
            # ç®€å•çš„transformerå—
            self.norm = nn.LayerNorm(vision_config['hidden_size'])
            self.output_projection = nn.Linear(
                vision_config['hidden_size'], 
                vision_config['out_hidden_size']
            )
            
        def forward(self, pixel_values):
            # ç®€å•çš„å‰å‘ä¼ æ’­
            B, C, H, W = pixel_values.shape
            x = self.patch_embed(pixel_values)  # [B, hidden_size, H', W']
            x = x.flatten(2).transpose(1, 2)     # [B, num_patches, hidden_size]
            x = self.norm(x)
            x = self.output_projection(x)
            return x
    
    return QwenVisionEncoder(vision_config)

def test_qwen_vision_encoder():
    """æµ‹è¯•Qwen2.5-VLè§†è§‰ç¼–ç å™¨çš„å¯¼å…¥å’ŒåŸºæœ¬åŠŸèƒ½"""
    
    print("ğŸš€ å¼€å§‹æµ‹è¯•Qwen2.5-VLè§†è§‰ç¼–ç å™¨...")
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸ“± ä½¿ç”¨è®¾å¤‡: {device}")
    
    try:
        # 1. åŠ è½½é…ç½®
        print("ğŸ“¥ åŠ è½½Qwen2.5-VLé…ç½®...")
        config = load_qwen_vision_config()
        vision_config = config['vision_config']
        
        print(f"ğŸ”§ è§†è§‰é…ç½®:")
        print(f"  - æ·±åº¦: {vision_config['depth']}")
        print(f"  - éšè—ç»´åº¦: {vision_config['hidden_size']}")
        print(f"  - è¾“å‡ºç»´åº¦: {vision_config['out_hidden_size']}")
        print(f"  - Patchå¤§å°: {vision_config['patch_size']}")
        
        # 2. åˆ›å»ºç®€åŒ–çš„è§†è§‰ç¼–ç å™¨
        print("\nğŸ—ï¸ åˆ›å»ºè§†è§‰ç¼–ç å™¨...")
        vision_encoder = create_qwen_vision_encoder(config)
        vision_encoder = vision_encoder.to(device)
        
        print(f"âœ… è§†è§‰ç¼–ç å™¨åˆ›å»ºæˆåŠŸ!")
        print(f"ğŸ“Š å‚æ•°é‡: {sum(p.numel() for p in vision_encoder.parameters()):,}")
        
        # 3. åŠ è½½é¢„å¤„ç†å™¨
        print("\nğŸ“¥ åŠ è½½å›¾åƒé¢„å¤„ç†å™¨...")
        try:
            processor = AutoImageProcessor.from_pretrained("/home/Qwen2.5-VL-3B-Instruct")
        except:
            # å¦‚æœå¤±è´¥ï¼Œä½¿ç”¨ç®€å•çš„é¢„å¤„ç†
            print("âš ï¸ ä½¿ç”¨ç®€å•é¢„å¤„ç†å™¨")
            processor = None
        
        # 4. æµ‹è¯•è§†è§‰ç¼–ç å™¨
        print("\nğŸ” æµ‹è¯•è§†è§‰ç¼–ç å™¨åŠŸèƒ½...")
        
        # åˆ›å»ºæµ‹è¯•å›¾åƒ
        test_image = torch.randn(1, 3, 224, 224).to(device)
        
        print(f"ğŸ“¥ è¾“å…¥å›¾åƒå°ºå¯¸: {test_image.shape}")
        
        # æå–è§†è§‰ç‰¹å¾
        with torch.no_grad():
            visual_features = vision_encoder(test_image)
        
        print(f"ğŸ“¤ è¾“å‡ºç‰¹å¾å°ºå¯¸: {visual_features.shape}")
        print(f"ğŸ“Š ç‰¹å¾ç»Ÿè®¡: mean={visual_features.mean().item():.4f}, std={visual_features.std().item():.4f}")
        
        # 5. æµ‹è¯•æ‰¹å¤„ç†
        print("\nğŸ”„ æµ‹è¯•æ‰¹å¤„ç†...")
        batch_images = torch.randn(2, 3, 224, 224).to(device)
        
        with torch.no_grad():
            batch_visual_features = vision_encoder(batch_images)
        
        print(f"ğŸ“¥ æ‰¹å¤„ç†è¾“å…¥å°ºå¯¸: {batch_images.shape}")
        print(f"ğŸ“¤ æ‰¹å¤„ç†è¾“å‡ºå°ºå¯¸: {batch_visual_features.shape}")
        
        print("\nâœ… Qwen2.5-VLè§†è§‰ç¼–ç å™¨æµ‹è¯•é€šè¿‡!")
        
        return {
            'vision_encoder': vision_encoder,
            'processor': processor,
            'device': device,
            'output_dim': visual_features.shape[-1],
            'config': config
        }
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return None

if __name__ == "__main__":
    result = test_qwen_vision_encoder()
    if result:
        print(f"\nğŸ‰ æµ‹è¯•æˆåŠŸ! è§†è§‰ç‰¹å¾ç»´åº¦: {result['output_dim']}")
    else:
        print("\nğŸ’¥ æµ‹è¯•å¤±è´¥!")
        sys.exit(1)
